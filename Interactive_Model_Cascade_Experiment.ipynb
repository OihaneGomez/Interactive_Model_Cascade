{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Set global random seed for reproducibility\n",
    "# ------------------------------------------\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '1'\n",
    "\n",
    "import random as rn\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "rn.seed(1)\n",
    "\n",
    "# ------------------------------------------\n",
    "# Suppress warnings for cleaner outputs\n",
    "# ------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# General-purpose libraries\n",
    "# ------------------------------------------\n",
    "import sys\n",
    "import glob\n",
    "import gc\n",
    "import ast\n",
    "import itertools\n",
    "\n",
    "# ------------------------------------------\n",
    "# Data handling and visualization\n",
    "# ------------------------------------------\n",
    "import pandas as pd\n",
    "import openpyxl  # Ensure openpyxl is installed and imported\n",
    "\n",
    "# ------------------------------------------\n",
    "# Feature extraction and preprocessing\n",
    "# ------------------------------------------\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import kurtosis, skew\n",
    "from numpy import sqrt, argmax\n",
    "\n",
    "# ------------------------------------------\n",
    "# Dimensionality reduction & visualization\n",
    "# ------------------------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ------------------------------------------\n",
    "# Model selection and evaluation\n",
    "# ------------------------------------------\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedShuffleSplit,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    LeaveOneGroupOut,\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    "    validation_curve,\n",
    "    GridSearchCV,\n",
    "    learning_curve\n",
    ")\n",
    "\n",
    "# ------------------------------------------\n",
    "# Classifiers\n",
    "# ------------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm, tree\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# ------------------------------------------\n",
    "# Evaluation metrics\n",
    "# ------------------------------------------\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Feature selection\n",
    "# ------------------------------------------\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    chi2,\n",
    "    SelectPercentile,\n",
    "    f_classif,\n",
    "    RFECV\n",
    ")\n",
    "\n",
    "# ------------------------------------------\n",
    "# Sklearn utilities\n",
    "# ------------------------------------------\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Helper function to extract subject ID\n",
    "# ------------------------------------------\n",
    "def obtain_subject(f):\n",
    "    \"\"\"\n",
    "    Extracts the subject ID (e.g., 'M1', 'F4') from the filename.\n",
    "    \n",
    "    Assumes filenames follow a pattern like:\n",
    "    'Other_M1_Glass_Sit_12_02-11-2020_11-04-49.txt'\n",
    "\n",
    "    Parameters:\n",
    "        f (str): Full path or filename of the data file.\n",
    "\n",
    "    Returns:\n",
    "        str: The subject ID string.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(f)\n",
    "    parts = filename.split('_')\n",
    "    return parts[1]  # e.g., \"M1\", \"F2\", etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Variables\n",
    "\n",
    "    \n",
    "initial component list, number_initial_components:\n",
    "Defines the complete list of available signals and selects how many of them to consider for processing.\n",
    "\n",
    "best_features_number:\n",
    "Sets the maximum number of features to process (e.g., 486 features for 9 signals).\n",
    "\n",
    "Dnumber_of_segments:\n",
    "Determines the number of subwindows (segments) each data example is divided into.\n",
    "\n",
    "n_times:\n",
    "Sets how many times the experiment is repeated, with results being averaged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All signal components: ['x', 'y', 'z', 'gyrox', 'gyroy', 'gyroz', 'pitch', 'roll', 'yaw']\n",
      "Selected components: ['x', 'y', 'z', 'gyrox', 'gyroy', 'gyroz', 'pitch', 'roll', 'yaw']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Signal Configuration\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Number of signal components to use (e.g., 3 for XYZ, 9 for all)\n",
    "number_initial_components = 9\n",
    "\n",
    "# Full list of available raw signal components\n",
    "initial_componet_list = [\"x\", \"y\", \"z\", \"gyrox\", \"gyroy\", \"gyroz\", \"pitch\", \"roll\", \"yaw\"]\n",
    "\n",
    "# Subset of components actually used for feature extraction\n",
    "initial_data_reduced_names = initial_componet_list[:number_initial_components]\n",
    "\n",
    "# Display signal configuration\n",
    "print(\"All signal components:\", initial_componet_list)\n",
    "print(\"Selected components:\", initial_data_reduced_names)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Feature Selection Configuration\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Maximum number of features to retain (e.g., 486 for 9 components)\n",
    "# - Max 162 for XYZ (3 signals)\n",
    "# - Max 54 for X only\n",
    "best_features_number = 486\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Segmentation Parameters\n",
    "# ------------------------------------------------------------\n",
    "number_of_segments = 5\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Dataset Path Configuration\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Relative path to dataset folder\n",
    "root = './OHM_Dataset_3class/'  # Regular Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Mapping and Feature Extraction\n",
    "\n",
    "Defines the activity label mapping (assigning numeric codes to each activity), loads and processes data files for each activity class using the gather() function, and implements feature extraction routines. It includes helper functions for median filtering with a sliding window (strided_app()) and for extracting statistical features (both over the whole signal and segmented subwindows) from each file, while also appending class and subject identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity labels: ['Other', 'Drink_glass', 'Drink_bottle']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Activity Class Mapping\n",
    "# ------------------------------------------------------------\n",
    "# Dictionary mapping activity names to numeric labels\n",
    "wrist_class = {\n",
    "    'Other': 0,\n",
    "    'Drink_glass': 1,\n",
    "    'Drink_bottle': 2\n",
    "}\n",
    "\n",
    "# Sorted list of class labels\n",
    "wrist_labels = sorted(wrist_class, key=lambda x: x[1], reverse=True)\n",
    "labels = wrist_labels\n",
    "\n",
    "print(\"Activity labels:\", wrist_labels)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load and Process All Files for Each Class\n",
    "# ------------------------------------------------------------\n",
    "def gather(class_dict, split_index, segments):\n",
    "    \"\"\"\n",
    "    Imports .txt files per class, computes features, and returns a combined DataFrame.\n",
    "    \"\"\"\n",
    "    df = []\n",
    "    for c in class_dict.keys():\n",
    "        f = glob.glob(root + c + '/*')  # Get all files in class folder\n",
    "        d = pd.DataFrame(reformat(f, cls=c, split_index=split_index, segments=segments))  # Extract features\n",
    "        print(\"Processed class:\", c)\n",
    "        df.append(d)\n",
    "    return pd.concat(df)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Median Filtering with Striding (used for noise smoothing)\n",
    "# ------------------------------------------------------------\n",
    "def strided_app(a, L, S):\n",
    "    \"\"\"\n",
    "    Applies a sliding window (stride trick) for median filtering.\n",
    "    L: window length, S: stride step.\n",
    "    \"\"\"\n",
    "    nrows = ((a.size - L) // S) + 1\n",
    "    n = a.strides[0]\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Extract Features from Each File\n",
    "# ------------------------------------------------------------\n",
    "def reformat(files, cls, split_index, segments):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from sensor data, optionally segmented.\n",
    "    \"\"\"\n",
    "    big_list = []\n",
    "    \n",
    "    for f in files:\n",
    "        subject_id = obtain_subject(f)\n",
    "        \n",
    "        # Read signal data with predefined columns\n",
    "        data = pd.read_csv(f, sep=',', header=None, names=[\n",
    "            'x', 'y', 'z', 'gyrox', 'gyroy', 'gyroz', 'pitch', 'roll', 'yaw'\n",
    "        ])\n",
    "\n",
    "        # Extract individual axis columns\n",
    "        df_x, df_y, df_z = data.iloc[:, 0:1], data.iloc[:, 1:2], data.iloc[:, 2:3]\n",
    "        df_gyrox, df_gyroy, df_gyroz = data.iloc[:, 3:4], data.iloc[:, 4:5], data.iloc[:, 5:6]\n",
    "        df_pitch, df_roll, df_yaw = data.iloc[:, 6:7], data.iloc[:, 7:8], data.iloc[:, 8:9]\n",
    "\n",
    "        # Apply median filtering (window = 3, stride = 1)\n",
    "        x = np.median(strided_app(df_x.values.flatten(), 3, 1), axis=1)\n",
    "        y = np.median(strided_app(df_y.values.flatten(), 3, 1), axis=1)\n",
    "        z = np.median(strided_app(df_z.values.flatten(), 3, 1), axis=1)\n",
    "        gyrox = np.median(strided_app(df_gyrox.values.flatten(), 3, 1), axis=1)\n",
    "        gyroy = np.median(strided_app(df_gyroy.values.flatten(), 3, 1), axis=1)\n",
    "        gyroz = np.median(strided_app(df_gyroz.values.flatten(), 3, 1), axis=1)\n",
    "        pitch = np.median(strided_app(df_pitch.values.flatten(), 3, 1), axis=1)\n",
    "        roll = np.median(strided_app(df_roll.values.flatten(), 3, 1), axis=1)\n",
    "        yaw = np.median(strided_app(df_yaw.values.flatten(), 3, 1), axis=1)\n",
    "\n",
    "        # Concatenate all signals into one DataFrame\n",
    "        data_all = pd.concat([\n",
    "            df_x.reset_index(drop=True), df_y.reset_index(drop=True), df_z.reset_index(drop=True),\n",
    "            df_gyrox.reset_index(drop=True), df_gyroy.reset_index(drop=True), df_gyroz.reset_index(drop=True),\n",
    "            df_pitch.reset_index(drop=True), df_roll.reset_index(drop=True), df_yaw.reset_index(drop=True)\n",
    "        ], axis=1)\n",
    "\n",
    "        # Select only the first N components\n",
    "        data_reduced = data_all.iloc[:, :number_initial_components]\n",
    "\n",
    "        # Split the time series into segments\n",
    "        data_split = np.array_split(data_reduced, split_index)\n",
    "\n",
    "        appended_features = []\n",
    "\n",
    "        # Whole-window features\n",
    "        features_whole = pd.concat([\n",
    "            data_reduced.mean(axis=0).rename(index=lambda x: 'mean_' + x),\n",
    "            data_reduced.std(axis=0).rename(index=lambda x: 'std_' + x),\n",
    "            data_reduced.median(axis=0).rename(index=lambda x: 'median_' + x),\n",
    "            data_reduced.sub(data_reduced.mean()).abs().mean().rename(index=lambda x: 'mad_' + x),\n",
    "            data_reduced.max(axis=0).rename(index=lambda x: 'max_' + x),\n",
    "            data_reduced.kurtosis(axis=0).rename(index=lambda x: 'kur_' + x),\n",
    "            data_reduced.skew(axis=0).rename(index=lambda x: 'skw_' + x),\n",
    "            data_reduced.var(axis=0).rename(index=lambda x: 'var_' + x),\n",
    "            data_reduced.min(axis=0).rename(index=lambda x: 'min_' + x)\n",
    "        ])\n",
    "\n",
    "        # Segment-wise features\n",
    "        for i in range(split_index):\n",
    "            features = pd.concat([\n",
    "                data_split[i].mean(axis=0).rename(index=lambda x: f'mean_{x}_{i}'),\n",
    "                data_split[i].std(axis=0).rename(index=lambda x: f'std_{x}_{i}'),\n",
    "                data_split[i].median(axis=0).rename(index=lambda x: f'median_{x}_{i}'),\n",
    "                data_split[i].sub(data_split[i].mean()).abs().mean().rename(index=lambda x: f'mad_{x}_{i}'),\n",
    "                data_split[i].max(axis=0).rename(index=lambda x: f'max_{x}_{i}'),\n",
    "                data_split[i].kurtosis(axis=0).rename(index=lambda x: f'kur_{x}_{i}'),\n",
    "                data_split[i].skew(axis=0).rename(index=lambda x: f'skw_{x}_{i}'),\n",
    "                data_split[i].var(axis=0).rename(index=lambda x: f'var_{x}_{i}'),\n",
    "                data_split[i].min(axis=0).rename(index=lambda x: f'min_{x}_{i}')\n",
    "            ])\n",
    "            appended_features.append(features)\n",
    "\n",
    "        # Combine features into final vector\n",
    "        if segments is False:\n",
    "            appended_features_all = pd.concat([features_whole])\n",
    "        else:\n",
    "            appended_features_all = pd.concat([features_whole])\n",
    "            for i in range(split_index):\n",
    "                appended_features_all = pd.concat([appended_features_all, appended_features[i]])\n",
    "\n",
    "        # Add class and subject info\n",
    "        appended_features_all['Y'] = wrist_class[cls]\n",
    "        appended_features_all['Subject'] = subject_id\n",
    "\n",
    "        big_list.append(appended_features_all)\n",
    "\n",
    "    # Return list of feature vectors (one per sequence)\n",
    "    return big_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matrix Extraction\n",
    "Uses gather() function to extract features from all classes. It then isolates target labels (Y) and creates the features matrix (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed class: Other\n",
      "Processed class: Drink_glass\n",
      "Processed class: Drink_bottle\n",
      "Total features: 487\n",
      "Feature matrix shape: (1000, 486)\n",
      "Feature names: ['mean_x', 'mean_y', 'mean_z', 'mean_gyrox', 'mean_gyroy', 'mean_gyroz', 'mean_pitch', 'mean_roll', 'mean_yaw', 'std_x', 'std_y', 'std_z', 'std_gyrox', 'std_gyroy', 'std_gyroz', 'std_pitch', 'std_roll', 'std_yaw', 'median_x', 'median_y', 'median_z', 'median_gyrox', 'median_gyroy', 'median_gyroz', 'median_pitch', 'median_roll', 'median_yaw', 'mad_x', 'mad_y', 'mad_z', 'mad_gyrox', 'mad_gyroy', 'mad_gyroz', 'mad_pitch', 'mad_roll', 'mad_yaw', 'max_x', 'max_y', 'max_z', 'max_gyrox', 'max_gyroy', 'max_gyroz', 'max_pitch', 'max_roll', 'max_yaw', 'kur_x', 'kur_y', 'kur_z', 'kur_gyrox', 'kur_gyroy', 'kur_gyroz', 'kur_pitch', 'kur_roll', 'kur_yaw', 'skw_x', 'skw_y', 'skw_z', 'skw_gyrox', 'skw_gyroy', 'skw_gyroz', 'skw_pitch', 'skw_roll', 'skw_yaw', 'var_x', 'var_y', 'var_z', 'var_gyrox', 'var_gyroy', 'var_gyroz', 'var_pitch', 'var_roll', 'var_yaw', 'min_x', 'min_y', 'min_z', 'min_gyrox', 'min_gyroy', 'min_gyroz', 'min_pitch', 'min_roll', 'min_yaw', 'mean_x_0', 'mean_y_0', 'mean_z_0', 'mean_gyrox_0', 'mean_gyroy_0', 'mean_gyroz_0', 'mean_pitch_0', 'mean_roll_0', 'mean_yaw_0', 'std_x_0', 'std_y_0', 'std_z_0', 'std_gyrox_0', 'std_gyroy_0', 'std_gyroz_0', 'std_pitch_0', 'std_roll_0', 'std_yaw_0', 'median_x_0', 'median_y_0', 'median_z_0', 'median_gyrox_0', 'median_gyroy_0', 'median_gyroz_0', 'median_pitch_0', 'median_roll_0', 'median_yaw_0', 'mad_x_0', 'mad_y_0', 'mad_z_0', 'mad_gyrox_0', 'mad_gyroy_0', 'mad_gyroz_0', 'mad_pitch_0', 'mad_roll_0', 'mad_yaw_0', 'max_x_0', 'max_y_0', 'max_z_0', 'max_gyrox_0', 'max_gyroy_0', 'max_gyroz_0', 'max_pitch_0', 'max_roll_0', 'max_yaw_0', 'kur_x_0', 'kur_y_0', 'kur_z_0', 'kur_gyrox_0', 'kur_gyroy_0', 'kur_gyroz_0', 'kur_pitch_0', 'kur_roll_0', 'kur_yaw_0', 'skw_x_0', 'skw_y_0', 'skw_z_0', 'skw_gyrox_0', 'skw_gyroy_0', 'skw_gyroz_0', 'skw_pitch_0', 'skw_roll_0', 'skw_yaw_0', 'var_x_0', 'var_y_0', 'var_z_0', 'var_gyrox_0', 'var_gyroy_0', 'var_gyroz_0', 'var_pitch_0', 'var_roll_0', 'var_yaw_0', 'min_x_0', 'min_y_0', 'min_z_0', 'min_gyrox_0', 'min_gyroy_0', 'min_gyroz_0', 'min_pitch_0', 'min_roll_0', 'min_yaw_0', 'mean_x_1', 'mean_y_1', 'mean_z_1', 'mean_gyrox_1', 'mean_gyroy_1', 'mean_gyroz_1', 'mean_pitch_1', 'mean_roll_1', 'mean_yaw_1', 'std_x_1', 'std_y_1', 'std_z_1', 'std_gyrox_1', 'std_gyroy_1', 'std_gyroz_1', 'std_pitch_1', 'std_roll_1', 'std_yaw_1', 'median_x_1', 'median_y_1', 'median_z_1', 'median_gyrox_1', 'median_gyroy_1', 'median_gyroz_1', 'median_pitch_1', 'median_roll_1', 'median_yaw_1', 'mad_x_1', 'mad_y_1', 'mad_z_1', 'mad_gyrox_1', 'mad_gyroy_1', 'mad_gyroz_1', 'mad_pitch_1', 'mad_roll_1', 'mad_yaw_1', 'max_x_1', 'max_y_1', 'max_z_1', 'max_gyrox_1', 'max_gyroy_1', 'max_gyroz_1', 'max_pitch_1', 'max_roll_1', 'max_yaw_1', 'kur_x_1', 'kur_y_1', 'kur_z_1', 'kur_gyrox_1', 'kur_gyroy_1', 'kur_gyroz_1', 'kur_pitch_1', 'kur_roll_1', 'kur_yaw_1', 'skw_x_1', 'skw_y_1', 'skw_z_1', 'skw_gyrox_1', 'skw_gyroy_1', 'skw_gyroz_1', 'skw_pitch_1', 'skw_roll_1', 'skw_yaw_1', 'var_x_1', 'var_y_1', 'var_z_1', 'var_gyrox_1', 'var_gyroy_1', 'var_gyroz_1', 'var_pitch_1', 'var_roll_1', 'var_yaw_1', 'min_x_1', 'min_y_1', 'min_z_1', 'min_gyrox_1', 'min_gyroy_1', 'min_gyroz_1', 'min_pitch_1', 'min_roll_1', 'min_yaw_1', 'mean_x_2', 'mean_y_2', 'mean_z_2', 'mean_gyrox_2', 'mean_gyroy_2', 'mean_gyroz_2', 'mean_pitch_2', 'mean_roll_2', 'mean_yaw_2', 'std_x_2', 'std_y_2', 'std_z_2', 'std_gyrox_2', 'std_gyroy_2', 'std_gyroz_2', 'std_pitch_2', 'std_roll_2', 'std_yaw_2', 'median_x_2', 'median_y_2', 'median_z_2', 'median_gyrox_2', 'median_gyroy_2', 'median_gyroz_2', 'median_pitch_2', 'median_roll_2', 'median_yaw_2', 'mad_x_2', 'mad_y_2', 'mad_z_2', 'mad_gyrox_2', 'mad_gyroy_2', 'mad_gyroz_2', 'mad_pitch_2', 'mad_roll_2', 'mad_yaw_2', 'max_x_2', 'max_y_2', 'max_z_2', 'max_gyrox_2', 'max_gyroy_2', 'max_gyroz_2', 'max_pitch_2', 'max_roll_2', 'max_yaw_2', 'kur_x_2', 'kur_y_2', 'kur_z_2', 'kur_gyrox_2', 'kur_gyroy_2', 'kur_gyroz_2', 'kur_pitch_2', 'kur_roll_2', 'kur_yaw_2', 'skw_x_2', 'skw_y_2', 'skw_z_2', 'skw_gyrox_2', 'skw_gyroy_2', 'skw_gyroz_2', 'skw_pitch_2', 'skw_roll_2', 'skw_yaw_2', 'var_x_2', 'var_y_2', 'var_z_2', 'var_gyrox_2', 'var_gyroy_2', 'var_gyroz_2', 'var_pitch_2', 'var_roll_2', 'var_yaw_2', 'min_x_2', 'min_y_2', 'min_z_2', 'min_gyrox_2', 'min_gyroy_2', 'min_gyroz_2', 'min_pitch_2', 'min_roll_2', 'min_yaw_2', 'mean_x_3', 'mean_y_3', 'mean_z_3', 'mean_gyrox_3', 'mean_gyroy_3', 'mean_gyroz_3', 'mean_pitch_3', 'mean_roll_3', 'mean_yaw_3', 'std_x_3', 'std_y_3', 'std_z_3', 'std_gyrox_3', 'std_gyroy_3', 'std_gyroz_3', 'std_pitch_3', 'std_roll_3', 'std_yaw_3', 'median_x_3', 'median_y_3', 'median_z_3', 'median_gyrox_3', 'median_gyroy_3', 'median_gyroz_3', 'median_pitch_3', 'median_roll_3', 'median_yaw_3', 'mad_x_3', 'mad_y_3', 'mad_z_3', 'mad_gyrox_3', 'mad_gyroy_3', 'mad_gyroz_3', 'mad_pitch_3', 'mad_roll_3', 'mad_yaw_3', 'max_x_3', 'max_y_3', 'max_z_3', 'max_gyrox_3', 'max_gyroy_3', 'max_gyroz_3', 'max_pitch_3', 'max_roll_3', 'max_yaw_3', 'kur_x_3', 'kur_y_3', 'kur_z_3', 'kur_gyrox_3', 'kur_gyroy_3', 'kur_gyroz_3', 'kur_pitch_3', 'kur_roll_3', 'kur_yaw_3', 'skw_x_3', 'skw_y_3', 'skw_z_3', 'skw_gyrox_3', 'skw_gyroy_3', 'skw_gyroz_3', 'skw_pitch_3', 'skw_roll_3', 'skw_yaw_3', 'var_x_3', 'var_y_3', 'var_z_3', 'var_gyrox_3', 'var_gyroy_3', 'var_gyroz_3', 'var_pitch_3', 'var_roll_3', 'var_yaw_3', 'min_x_3', 'min_y_3', 'min_z_3', 'min_gyrox_3', 'min_gyroy_3', 'min_gyroz_3', 'min_pitch_3', 'min_roll_3', 'min_yaw_3', 'mean_x_4', 'mean_y_4', 'mean_z_4', 'mean_gyrox_4', 'mean_gyroy_4', 'mean_gyroz_4', 'mean_pitch_4', 'mean_roll_4', 'mean_yaw_4', 'std_x_4', 'std_y_4', 'std_z_4', 'std_gyrox_4', 'std_gyroy_4', 'std_gyroz_4', 'std_pitch_4', 'std_roll_4', 'std_yaw_4', 'median_x_4', 'median_y_4', 'median_z_4', 'median_gyrox_4', 'median_gyroy_4', 'median_gyroz_4', 'median_pitch_4', 'median_roll_4', 'median_yaw_4', 'mad_x_4', 'mad_y_4', 'mad_z_4', 'mad_gyrox_4', 'mad_gyroy_4', 'mad_gyroz_4', 'mad_pitch_4', 'mad_roll_4', 'mad_yaw_4', 'max_x_4', 'max_y_4', 'max_z_4', 'max_gyrox_4', 'max_gyroy_4', 'max_gyroz_4', 'max_pitch_4', 'max_roll_4', 'max_yaw_4', 'kur_x_4', 'kur_y_4', 'kur_z_4', 'kur_gyrox_4', 'kur_gyroy_4', 'kur_gyroz_4', 'kur_pitch_4', 'kur_roll_4', 'kur_yaw_4', 'skw_x_4', 'skw_y_4', 'skw_z_4', 'skw_gyrox_4', 'skw_gyroy_4', 'skw_gyroz_4', 'skw_pitch_4', 'skw_roll_4', 'skw_yaw_4', 'var_x_4', 'var_y_4', 'var_z_4', 'var_gyrox_4', 'var_gyroy_4', 'var_gyroz_4', 'var_pitch_4', 'var_roll_4', 'var_yaw_4', 'min_x_4', 'min_y_4', 'min_z_4', 'min_gyrox_4', 'min_gyroy_4', 'min_gyroz_4', 'min_pitch_4', 'min_roll_4', 'min_yaw_4', 'Y']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Display Configuration for DataFrames\n",
    "# ------------------------------------------------------------\n",
    "pd.set_option('display.max_columns', None)     # Show all columns\n",
    "pd.set_option('display.max_rows', None)        # Show all rows\n",
    "pd.set_option('display.max_colwidth', None)    # Don't truncate column content\n",
    "\n",
    "\n",
    "# Determine if segmentation is enabled\n",
    "segments = number_of_segments > 1\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Feature Extraction from All Classes\n",
    "# ------------------------------------------------------------\n",
    "wrist_df = gather(wrist_class, segments=segments, split_index=number_of_segments)\n",
    "\n",
    "# Extract target labels (Y)\n",
    "wrist_Y = np.asarray(wrist_df.Y)\n",
    "n_samples = len(wrist_Y)\n",
    "\n",
    "# Determine number of features (excluding label)\n",
    "num_features = len(wrist_df.columns) - 1\n",
    "print('Total features:', num_features)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Extract Features Matrix (X)\n",
    "# ------------------------------------------------------------\n",
    "# NOTE: last column is assumed to be 'Subject', so it's excluded\n",
    "wrist_X = np.asarray(wrist_df.iloc[:, :num_features - 1])\n",
    "\n",
    "# Optional normalization \n",
    "\"\"\"\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "wrist_X = min_max_scaler.fit_transform(wrist_df.iloc[:, :num_features])\n",
    "\"\"\"\n",
    "\n",
    "# Show feature matrix dimensions and sample\n",
    "print(\"Feature matrix shape:\", wrist_X.shape)\n",
    "\n",
    "\n",
    "# Show feature names used\n",
    "all_features = num_features\n",
    "print(\"Feature names:\", wrist_df.iloc[:, :num_features].columns.values.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Setup and Performance Tracking Structures\n",
    "Initializes the classifiers and sets up the data structures needed to track performance metrics during model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Classifier Setup and Performance Tracking Structures\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "n = 0  # Fixed random seed\n",
    "\n",
    "# List of classifiers (order matches `alg_array_names`)\n",
    "alg_array = [\n",
    "    LogisticRegression(random_state=n),\n",
    "    RandomForestClassifier(n_estimators=100, verbose=0, random_state=n),\n",
    "    KNeighborsClassifier(n_neighbors=3),\n",
    "    GaussianNB(),\n",
    "    svm.SVC(kernel='linear', C=64.0, random_state=n, probability=True),\n",
    "    MLPClassifier(hidden_layer_sizes=(16, 16, 16), max_iter=1000, random_state=n),\n",
    "    tree.DecisionTreeClassifier(random_state=n)\n",
    "]\n",
    "\n",
    "# Short names used for tracking each classifier\n",
    "alg_array_names = ['LG', 'RF', 'KNN', 'NB', 'SVM', 'MLP', 'DT']\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialize performance metric storage\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# F1 score arrays (pre-threshold and final)\n",
    "f1_score_array_LG_pre, f1_score_array_RF_pre, f1_score_array_KNN_pre, f1_score_array_NB_pre, f1_score_array_SVM_pre, f1_score_array_MLP_pre, f1_score_array_DT_pre = [], [], [], [], [], [], []\n",
    "f1_score_array_LG, f1_score_array_RF, f1_score_array_KNN, f1_score_array_NB, f1_score_array_SVM, f1_score_array_MLP, f1_score_array_DT = [], [], [], [], [], [], []\n",
    "\n",
    "# Errors (raw and thresholded)\n",
    "n_error_LG, n_error_RF, n_error_KNN, n_error_NB, n_error_SVM, n_error_MLP, n_error_DT = [], [], [], [], [], [], []\n",
    "n_error_LG_tres, n_error_RF_tres, n_error_KNN_tres, n_error_NB_tres, n_error_SVM_tres, n_error_MLP_tres, n_error_DT_tres = [], [], [], [], [], [], []\n",
    "\n",
    "# Thresholds\n",
    "LG_tres, RF_tres, KNN_tres, NB_tres, SVM_tres, MLP_tres, DT_tres = [], [], [], [], [], [], []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Map metrics to variable names\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "array_dict_pre = {\n",
    "    \"LG\": f1_score_array_LG_pre,\n",
    "    \"RF\": f1_score_array_RF_pre,\n",
    "    \"KNN\": f1_score_array_KNN_pre,\n",
    "    \"NB\": f1_score_array_NB_pre,\n",
    "    \"SVM\": f1_score_array_SVM_pre,\n",
    "    \"MLP\": f1_score_array_MLP_pre,\n",
    "    \"DT\": f1_score_array_DT_pre\n",
    "}\n",
    "\n",
    "array_dict_pre_thres = dict(array_dict_pre)  # duplicate mapping for pre-threshold\n",
    "\n",
    "array_dict = {\n",
    "    \"LG\": f1_score_array_LG,\n",
    "    \"RF\": f1_score_array_RF,\n",
    "    \"KNN\": f1_score_array_KNN,\n",
    "    \"NB\": f1_score_array_NB,\n",
    "    \"SVM\": f1_score_array_SVM,\n",
    "    \"MLP\": f1_score_array_MLP,\n",
    "    \"DT\": f1_score_array_DT\n",
    "}\n",
    "\n",
    "error_dict = {\n",
    "    \"LG\": n_error_LG,\n",
    "    \"RF\": n_error_RF,\n",
    "    \"KNN\": n_error_KNN,\n",
    "    \"NB\": n_error_NB,\n",
    "    \"SVM\": n_error_SVM,\n",
    "    \"MLP\": n_error_MLP,\n",
    "    \"DT\": n_error_DT\n",
    "}\n",
    "\n",
    "error_dict_thres = {\n",
    "    \"LG\": n_error_LG_tres,\n",
    "    \"RF\": n_error_RF_tres,\n",
    "    \"KNN\": n_error_KNN_tres,\n",
    "    \"NB\": n_error_NB_tres,\n",
    "    \"SVM\": n_error_SVM_tres,\n",
    "    \"MLP\": n_error_MLP_tres,\n",
    "    \"DT\": n_error_DT_tres\n",
    "}\n",
    "\n",
    "threshold = {\n",
    "    \"LG\": LG_tres,\n",
    "    \"RF\": RF_tres,\n",
    "    \"KNN\": KNN_tres,\n",
    "    \"NB\": NB_tres,\n",
    "    \"SVM\": SVM_tres,\n",
    "    \"MLP\": MLP_tres,\n",
    "    \"DT\": DT_tres\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global evaluation metric arrays\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "recall_scores, precision_scores, accuracy_scores, f1_scores = [], [], [], []\n",
    "\n",
    "# Scoring functions to use in evaluation\n",
    "scoring = ['recall_macro', 'precision_macro', 'f1_macro', 'accuracy']\n",
    "\n",
    "# Link scoring metric names to their result containers\n",
    "array_score = {\n",
    "    'recall_macro': recall_scores,\n",
    "    'precision_macro': precision_scores,\n",
    "    'f1_macro': f1_scores,\n",
    "    'accuracy': accuracy_scores\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Performance Metric Arrays for Non-Segmented Setup (NO_S)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# F1 scores before thresholding\n",
    "f1_score_array_LG_pre_NO_S, f1_score_array_RF_pre_NO_S, f1_score_array_KNN_pre_NO_S, \\\n",
    "f1_score_array_NB_pre_NO_S, f1_score_array_SVM_pre_NO_S, f1_score_array_MLP_pre_NO_S, \\\n",
    "f1_score_array_DT_pre_NO_S = [], [], [], [], [], [], []\n",
    "\n",
    "# Classification errors (raw)\n",
    "n_error_LG_NO_S, n_error_RF_NO_S, n_error_KNN_NO_S, \\\n",
    "n_error_NB_NO_S, n_error_SVM_NO_S, n_error_MLP_NO_S, n_error_DT_NO_S = [], [], [], [], [], [], []\n",
    "\n",
    "# Classification errors (with thresholding)\n",
    "n_error_LG_tres_NO_S, n_error_RF_tres_NO_S, n_error_KNN_tres_NO_S, \\\n",
    "n_error_NB_tres_NO_S, n_error_SVM_tres_NO_S, n_error_MLP_tres_NO_S, n_error_DT_tres_NO_S = [], [], [], [], [], [], []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Map Metrics to Classifier Names\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Errors without thresholding\n",
    "error_dict_NO_S = {\n",
    "    \"LG\": n_error_LG_NO_S,\n",
    "    \"RF\": n_error_RF_NO_S,\n",
    "    \"KNN\": n_error_KNN_NO_S,\n",
    "    \"NB\": n_error_NB_NO_S,\n",
    "    \"SVM\": n_error_SVM_NO_S,\n",
    "    \"MLP\": n_error_MLP_NO_S,\n",
    "    \"DT\": n_error_DT_NO_S\n",
    "}\n",
    "\n",
    "# Errors with thresholding\n",
    "error_dict_thres_NO_S = {\n",
    "    \"LG\": n_error_LG_tres_NO_S,\n",
    "    \"RF\": n_error_RF_tres_NO_S,\n",
    "    \"KNN\": n_error_KNN_tres_NO_S,\n",
    "    \"NB\": n_error_NB_tres_NO_S,\n",
    "    \"SVM\": n_error_SVM_tres_NO_S,\n",
    "    \"MLP\": n_error_MLP_tres_NO_S,\n",
    "    \"DT\": n_error_DT_tres_NO_S\n",
    "}\n",
    "\n",
    "# F1 pre-threshold scores\n",
    "array_dict_pre_thres_NO_S = {\n",
    "    \"LG\": f1_score_array_LG_pre_NO_S,\n",
    "    \"RF\": f1_score_array_RF_pre_NO_S,\n",
    "    \"KNN\": f1_score_array_KNN_pre_NO_S,\n",
    "    \"NB\": f1_score_array_NB_pre_NO_S,\n",
    "    \"SVM\": f1_score_array_SVM_pre_NO_S,\n",
    "    \"MLP\": f1_score_array_MLP_pre_NO_S,\n",
    "    \"DT\": f1_score_array_DT_pre_NO_S\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Setup and Utility Functions\n",
    "Initializes settings for classification and defines a suite of utility functions to support model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Initial Setup\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "number_classes = len(wrist_labels)\n",
    "values_classes = np.unique(wrist_Y)\n",
    "start = values_classes[0]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Utility to detect missing class indices\n",
    "# ------------------------------------------------------------\n",
    "def missing_elements(L, labels, Y):\n",
    "    global start\n",
    "    end = Y[0]\n",
    "    if start == 0:\n",
    "        sort = sorted(set(range(0, len(labels))).difference(L))\n",
    "        for i in sort:\n",
    "            if i == end:\n",
    "                i = 0\n",
    "    else:\n",
    "        sort = sorted(set(range(1, len(labels) + 1)).difference(L))\n",
    "    return sort\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Custom LabelBinarizer supporting binary and multiclass\n",
    "# ------------------------------------------------------------\n",
    "class LabelBinarizer2:\n",
    "    def __init__(self):\n",
    "        self.lb = LabelBinarizer()\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.array(X)\n",
    "        self.lb.fit(X)\n",
    "        self.classes_ = self.lb.classes_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        X = np.array(X)\n",
    "        Xlb = self.lb.fit_transform(X)\n",
    "        self.classes_ = self.lb.classes_\n",
    "        if len(self.classes_) == 2:\n",
    "            Xlb = np.hstack((Xlb, 1 - Xlb))\n",
    "        return Xlb\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.array(X)\n",
    "        Xlb = self.lb.transform(X)\n",
    "        if len(self.classes_) == 2:\n",
    "            Xlb = np.hstack((Xlb, 1 - Xlb))\n",
    "        return Xlb\n",
    "\n",
    "    def inverse_transform(self, Xlb):\n",
    "        Xlb = np.array(Xlb)\n",
    "        if len(self.classes_) == 2:\n",
    "            return self.lb.inverse_transform(Xlb[:, 0])\n",
    "        return self.lb.inverse_transform(Xlb)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Adjust predicted arrays to ensure all class columns exist\n",
    "# ------------------------------------------------------------\n",
    "def complete_arrays(Y, pred, pred_binarized, proba, labels):\n",
    "    global start\n",
    "\n",
    "    # Ensure predicted binarized output has all classes\n",
    "    values_classes = np.unique(pred)\n",
    "    miss = missing_elements(values_classes, wrist_labels, Y)\n",
    "    for i in miss:\n",
    "        if start != 0:\n",
    "            i -= 1\n",
    "        pred_binarized = np.hstack((\n",
    "            pred_binarized[:, :i],\n",
    "            np.zeros((pred_binarized.shape[0], 1)),\n",
    "            pred_binarized[:, i:]\n",
    "        ))\n",
    "\n",
    "    # Build DataFrame of class probabilities\n",
    "    values_classes = np.unique(Y)\n",
    "    df = pd.DataFrame()\n",
    "    columns = labels\n",
    "    j = 0\n",
    "    for i in values_classes:\n",
    "        if start != 0:\n",
    "            i -= 1\n",
    "        df.insert(j, column=columns[int(i)], value=proba[:, int(i)])\n",
    "        j += 1\n",
    "\n",
    "    # Add missing columns (classes with 0 probability)\n",
    "    miss = missing_elements(values_classes, columns, wrist_Y)\n",
    "    for i in miss:\n",
    "        if start != 0:\n",
    "            i -= 1\n",
    "        df.insert(int(i), column=columns[int(i)], value=np.zeros(len(proba)))\n",
    "\n",
    "    return pred_binarized, df.values, df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main classification function (log-loss based)\n",
    "# ------------------------------------------------------------\n",
    "def classif_model(item, n_features, X_train, X_test, y_train, y_test, log_loss_T, final_model):\n",
    "    global n_samples, start\n",
    "    score_selection = []\n",
    "\n",
    "    # Feature selection\n",
    "    selection = SelectKBest(f_classif, k=n_features).fit(X_train, y_train)\n",
    "    X_train = selection.transform(X_train)\n",
    "    X_test = selection.transform(X_test)\n",
    "\n",
    "    # Train classifier and predict\n",
    "    model = item.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    proba = model.predict_proba(X_test)\n",
    "\n",
    "    # Label binarization\n",
    "    lb = LabelBinarizer2()\n",
    "    wrist_Y_binarized = lb.fit_transform(y_test)\n",
    "    pred_binarized = lb.fit_transform(pred)\n",
    "    pred_binarized, proba, df = complete_arrays(y_test, pred, pred_binarized, proba, wrist_labels)\n",
    "\n",
    "    # Compute log loss\n",
    "    ll_pred = multiclass_log_loss(pred_binarized, proba)  # You must have this function defined\n",
    "\n",
    "    # Build results DataFrame\n",
    "    df3 = pd.DataFrame(pred, columns=['Pred_label'])\n",
    "    df2 = pd.DataFrame(y_test, columns=['True_label'])\n",
    "    df = pd.DataFrame(proba, columns=labels).join(df2).join(df3)\n",
    "    df[\"LOG_LOSS\"] = ll_pred\n",
    "    df[\"Error_thres\"] = np.where(df[\"Pred_label\"] == df[\"True_label\"], '_', 'TRUE')\n",
    "\n",
    "    if not final_model:\n",
    "        df_thres_NO_S = df[df.LOG_LOSS <= log_loss_T]\n",
    "        df_pending_NO_S = df[df.LOG_LOSS > log_loss_T]\n",
    "        n_samples = df_pending_NO_S.shape[0]\n",
    "        index_selection = df_pending_NO_S.index.tolist()\n",
    "\n",
    "        errors_selection = df_thres_NO_S[df_thres_NO_S.Error_thres == 'TRUE'].shape[0]\n",
    "        if len(df_thres_NO_S):\n",
    "            clasif_report = classification_report(df_thres_NO_S[\"True_label\"], df_thres_NO_S[\"Pred_label\"], digits=4)\n",
    "            score_selection = f1_score(df_thres_NO_S[\"True_label\"], df_thres_NO_S[\"Pred_label\"], average=\"macro\")\n",
    "        else:\n",
    "            clasif_report = []\n",
    "            score_selection = 0\n",
    "    else:\n",
    "        df_thres_NO_S = df.copy()\n",
    "        index_selection = []\n",
    "        errors_selection = df_thres_NO_S[df_thres_NO_S.Error_thres == 'TRUE'].shape[0]\n",
    "        score_selection = f1_score(df[\"True_label\"], df[\"Pred_label\"], average=\"macro\")\n",
    "        clasif_report = classification_report(df[\"True_label\"], df[\"Pred_label\"], digits=4)\n",
    "\n",
    "    return index_selection, df_thres_NO_S, clasif_report, errors_selection, score_selection\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Variant that also returns indices of classified samples\n",
    "# ------------------------------------------------------------\n",
    "def classif_model_active(item, n_features, X_train, X_test, y_train, y_test, log_loss_T, final_model):\n",
    "    global n_samples, start\n",
    "    score_selection = []\n",
    "\n",
    "    # Feature selection\n",
    "    selection = SelectKBest(chi2, k=n_features).fit(X_train, y_train)\n",
    "    X_train = selection.transform(X_train)\n",
    "    X_test = selection.transform(X_test)\n",
    "\n",
    "    # Train and predict\n",
    "    model = item.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    proba = model.predict_proba(X_test)\n",
    "\n",
    "    # Label binarization\n",
    "    lb = LabelBinarizer2()\n",
    "    wrist_Y_binarized = lb.fit_transform(y_test)\n",
    "    pred_binarized = lb.fit_transform(pred)\n",
    "    pred_binarized, proba, df = complete_arrays(y_test, pred, pred_binarized, proba, wrist_labels)\n",
    "\n",
    "    # Compute log loss\n",
    "    ll_pred = multiclass_log_loss(pred_binarized, proba)\n",
    "\n",
    "    # Build results DataFrame\n",
    "    df3 = pd.DataFrame(pred, columns=['Pred_label'])\n",
    "    df2 = pd.DataFrame(y_test, columns=['True_label'])\n",
    "    df = pd.DataFrame(proba, columns=labels).join(df2).join(df3)\n",
    "    df[\"LOG_LOSS\"] = ll_pred\n",
    "    df[\"Error_thres\"] = np.where(df[\"Pred_label\"] == df[\"True_label\"], '_', 'TRUE')\n",
    "\n",
    "    if not final_model:\n",
    "        df_thres_NO_S = df[df.LOG_LOSS <= log_loss_T]\n",
    "        df_pending_NO_S = df[df.LOG_LOSS > log_loss_T]\n",
    "        n_samples = df_pending_NO_S.shape[0]\n",
    "\n",
    "        index_selection = df_pending_NO_S.index.tolist()\n",
    "        index_classified = df_thres_NO_S.index.tolist()\n",
    "\n",
    "        errors_selection = df_thres_NO_S[df_thres_NO_S.Error_thres == 'TRUE'].shape[0]\n",
    "        if len(df_thres_NO_S):\n",
    "            clasif_report = classification_report(df_thres_NO_S[\"True_label\"], df_thres_NO_S[\"Pred_label\"], digits=4)\n",
    "            score_selection = f1_score(df_thres_NO_S[\"True_label\"], df_thres_NO_S[\"Pred_label\"], average=\"macro\")\n",
    "        else:\n",
    "            clasif_report = []\n",
    "            score_selection = 0\n",
    "    else:\n",
    "        df_thres_NO_S = df.copy()\n",
    "        index_selection = []\n",
    "        index_classified = df_thres_NO_S.index.tolist()\n",
    "        errors_selection = df_thres_NO_S[df_thres_NO_S.Error_thres == 'TRUE'].shape[0]\n",
    "        score_selection = f1_score(df[\"True_label\"], df[\"Pred_label\"], average=\"macro\")\n",
    "        clasif_report = classification_report(df[\"True_label\"], df[\"Pred_label\"], digits=4)\n",
    "\n",
    "    return index_selection, index_classified, df_thres_NO_S, clasif_report, errors_selection, score_selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Interactive Model Cascade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment and Model Configuration\n",
    "\n",
    "Sets key parameters for the experiment:\n",
    "\n",
    "- Proportion_test = 0.4: Specifies that 40% of the data will be used for testing. This split ratio is used when partitioning the data for evaluation (e.g., during stratified splits).\n",
    "\n",
    "- n_times = 100: Sets the number of repetitions for the experiment.\n",
    "\n",
    "- n_features_mX: Configurates the cascade. \n",
    "\n",
    "\n",
    "    - Model 1: 10 features\n",
    "    - Model 2: 50 features\n",
    "    - Model 3: All features (486)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Other', 'Drink_glass', 'Drink_bottle']\n"
     ]
    }
   ],
   "source": [
    "# Fraction of data held out for validation out of the user instances (rest used for testing)\n",
    "proportion_test = 0.4  \n",
    "\n",
    "# Set the list of labels to be used (taken from wrist_labels)\n",
    "labels = wrist_labels\n",
    "print(labels)  # Print the labels for verification\n",
    "\n",
    "# Set the number of times to repeat the experiment and the number of cross-validation folds\n",
    "n_times = 100    # Number of repetitions for the experiment (for averaging results)\n",
    "\n",
    "# ---------------------------\n",
    "# Model 1 Configuration\n",
    "# ---------------------------\n",
    "n_features_m1 = 10   # Number of selected features for Model 1\n",
    "segments_m1 = True   # Flag indicating whether Model 1 will use segmented instances\n",
    "\n",
    "# ---------------------------\n",
    "# Model 2 Configuration\n",
    "# ---------------------------\n",
    "n_features_m2 = 50   # Number of selected features for Model 2\n",
    "segments_m2 = True   # Flag indicating whether Model 2 will use segmented instances\n",
    "\n",
    "# ---------------------------\n",
    "# Model 3 (Final Model) Configuration\n",
    "# ---------------------------\n",
    "n_features_m3 = 486  # Number of selected features for Model 3 (final model)\n",
    "segments_m3 = True   # Flag indicating whether Model 3 will use segmented instances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Configuration for Interactive Cascade Personalization\n",
    "\n",
    "This cell offers three alternative configurations for setting the confidence thresholds that govern when the cascade escalates to user input. According to the paper, the system operates with base thresholds—T1 = 0.15 (for the transition from Model 1 to Model 2) and T2 = 0.40 (for Model 2 to the interactive final stage). These thresholds determine whether a prediction is accepted automatically or sent for user labeling.\n",
    "\n",
    "User Involvement Levels:\n",
    "\n",
    "- High Involvement (Restrictive Configuration): Uses stricter (lower) thresholds by dividing the base values (T1/1.25 and T2/1.5). With lower thresholds, the cascade is less tolerant of uncertainty, causing more instances to be escalated for user labeling. This configuration leads to increased user interaction, providing more personalized feedback that can be used to retrain the models.\n",
    "\n",
    "- Medium Involvement (Base Configuration): Uses the base threshold values (T1 and T2). This represents a balanced scenario where the system moderately queries the user.\n",
    "\n",
    "- Low Involvement (Permissive Configuration): Uses higher thresholds by multiplying the base values (T1 × 1.25 and T2 × 1.5). With these higher thresholds, the system becomes more tolerant of uncertainty, resulting in fewer queries to the user and consequently less direct interaction.\n",
    "\n",
    "Usage Note:\n",
    "Only one threshold configuration cell should be active at a time. This allows you to simulate a specific degree of user involvement, matching the interactive cascade strategy described in the paper. The selected configuration directly affects the frequency of user queries and, ultimately, the level of personalization achieved through retraining with user-annotated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Intermediate Threshold\n",
    "# ------------------------------------------------------------------\n",
    "# Uncomment this cell (and comment out the others) when you want\n",
    "# to use an intermediate threshold configuration.\n",
    "\n",
    "# log_loss_T = 0.15\n",
    "# log_loss_T_sel = 0.4\n",
    "\n",
    "# file_name = \"Acc_ActiveLearning_Intermediate_T1_\" + str(log_loss_T) + \"_T2_\" + str(log_loss_T_sel) + \".xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Permissive thress Threshold\n",
    "# ------------------------------------------------------------------\n",
    "# Uncomment this cell (and comment out the others) when you want\n",
    "# to use a permissive threshold configuration.\n",
    "\n",
    "# log_loss_T = 0.15 * 1.25\n",
    "# log_loss_T_sel = 0.4 * 1.5\n",
    "\n",
    "# file_name = \"Acc_ActiveLearning_Permisive_T1_\" + str(log_loss_T) + \"_T2_\" + str(log_loss_T_sel) + \".xlsx\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "#restrictive Threshold\n",
    "# ------------------------------------------------------------------\n",
    "# Uncomment this cell (and comment out the others) when you want\n",
    "# to use a restrictive threshold configuration.\n",
    "\n",
    "log_loss_T = 0.15 / 1.25\n",
    "log_loss_T_sel = 0.4 / 1.5\n",
    "\n",
    "file_name = \"Acc_ActiveLearning_Restrictive_T1_\" + str(log_loss_T) + \"_T2_\" + str(log_loss_T_sel) + \".xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Cascade Evaluation and Retraining\n",
    "\n",
    "This cell implements the interactive cascade classification strategy as proposed in the paper *\"Integrating Personalization into Interactive Model Cascades for Efficient Human Activity Recognition at the Edge.\"* The code executes a series of nested loops that simulate the cascade's processing of uncertain predictions and the subsequent integration of user-labeled data for model personalization. Below is a summary of the main steps:\n",
    "\n",
    "1. **Cross-Validation Setup (LOSO):**  \n",
    "   - Uses Leave-One-Group-Out (LOO) cross-validation, where each subject (from `wrist_df[\"Subject\"].values`) is held out once.  \n",
    "   - This simulates a user-dependent evaluation (i.e., Leave-One-Subject-Out, LOSO) as described in the paper.\n",
    "\n",
    "2. **Baseline Performance Measurement:**  \n",
    "   - For each classifier (from `alg_array` with corresponding names in `alg_array_names`), the model is trained on the training data and its baseline predictions are computed on the test set.  \n",
    "   - Baseline accuracies and F1 scores are recorded for later comparison.\n",
    "\n",
    "3. **Interactive Cascade – Round One:**  \n",
    "   - The test set is further split via `StratifiedShuffleSplit` into two parts:  \n",
    "     - **First Split (`X_test_first`/`y_test_first`):**  \n",
    "       - Model 1 processes this subset, producing high-confidence predictions and identifying uncertain (\"pending\") instances.\n",
    "     - **Pending Processing (Model 2):**  \n",
    "       - The pending instances from Model 1 are further classified using Model 2.\n",
    "     - **Final Cascade Stage (Model 3):**  \n",
    "       - If uncertainty remains, Model 3 is applied, representing the final, automatic decision-making stage if no user interaction occurs.\n",
    "   - The performance (accuracy of Model 1+2 and Model 3 separately) is computed and stored.\n",
    "\n",
    "4. **User Feedback Simulation and Retraining (Round Two):**  \n",
    "   - The pending instances (those that Model 1+2 could not classify confidently) are conceptually “labeled by the user” and then added to the training set.\n",
    "   - The system performs a second round of training and evaluation on the updated dataset—this simulates the personalized retraining step described in the paper.\n",
    "   - Again, the metrics (accuracy, number of pending instances, etc.) are recorded.\n",
    "\n",
    "5. **Metrics Aggregation and Resetting:**  \n",
    "   - Across multiple iterations (`n_times` repetitions) and subjects (via LOSO cross-validation), performance metrics such as accuracy scores and pending instance counts are averaged.\n",
    "   - These results are stored in data structures (e.g., `df_results_save_final`) for later analysis, mirroring the comprehensive evaluation setup in the paper.\n",
    "   - Temporary arrays for each subject’s metrics are reset after processing, ensuring each iteration starts fresh.\n",
    "\n",
    "This evaluation procedure helps simulate the dual objectives of the paper:\n",
    "- **Minimizing unnecessary user interaction:** by confidently classifying data at early stages of the cascade.\n",
    "- **Enhancing personalization:** through retraining with user-supplied annotations when predictions fall below confidence thresholds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc_ActiveLearning_Restrictive_T1_0.12_T2_0.26666666666666666.xlsx\n",
      "----------------------------- \n",
      "---Number of Features 486-----\n",
      "----------------------------- \n",
      "\n",
      "\n",
      "\n",
      "########### LG ##########\n",
      "\n",
      "\n",
      "-------------------------- Subject 1 - F1 baseline max features 0.8590 \n",
      "-------------------------- Subject 1 - Acc baseline max features 0.8700 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def logloss(true_label, predicted, eps=1e-15):\n",
    "    p = np.clip(predicted, eps, 1 - eps)\n",
    "    if true_label == 1:\n",
    "        return -np.log(p)\n",
    "    else:\n",
    "        return -np.log(1 - p)\n",
    "\n",
    "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -(y_true * np.log(y_pred)).sum(axis=1)\n",
    "\n",
    "best_features_num = [486]  # [1, 3, 5, 10, 20, 50, all_features] or [100, 162, 324, 486]\n",
    "\n",
    "item_name = \"LG\"\n",
    "\n",
    "subject_name_df = []\n",
    "item_array_df = []\n",
    "result_baseline_df = []\n",
    "add_train = []  # Añadidas al train\n",
    "\n",
    "acc_M1_M2 = []  # Acc original train original\n",
    "acc_M3 = []     # Acc original train original\n",
    "\n",
    "second_round_acc_M1_M2 = []  # Acc train new\n",
    "second_round_acc_M3 = []       # Acc train new\n",
    "\n",
    "pending_M1_M2 = []  # Pending with original train\n",
    "pending_M1_M2_second_round = []  # Pending new train\n",
    "\n",
    "appended_data = []\n",
    "\n",
    "add_train_df = []  # Añadidas al train\n",
    "\n",
    "acc_M1_M2_df = []  # Acc original train original\n",
    "acc_M3_df = []     # Acc original train original\n",
    "\n",
    "second_round_acc_M1_M2_df = []  # Acc train new\n",
    "second_round_acc_M3_df = []      # Acc train new\n",
    "\n",
    "pending_M1_M2_df = []  # Pending with original train\n",
    "pending_M1_M2_second_round_df = []  # Pending new train\n",
    "\n",
    "error_dict_NO_S[item_name] = []\n",
    "error_dict_thres_NO_S[item_name] = []\n",
    "initial_scores_M1 = []\n",
    "initial_scores_M2 = []\n",
    "initial_scores_M3 = []\n",
    "threshold[item_name] = []\n",
    "error_dict[item_name] = []\n",
    "error_dict_thres[item_name] = []\n",
    "array_dict_pre[item_name] = []\n",
    "array_dict_pre_thres[item_name] = []\n",
    "error_dict[item_name] = []\n",
    "error_dict_thres[item_name] = []\n",
    "array_dict_pre[item_name] = []\n",
    "array_dict_pre_thres[item_name] = []\n",
    "\n",
    "final_selection_before_M3 = []\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "original_class = []\n",
    "predicted_class = []\n",
    "score_final = []\n",
    "score_final_M2 = []\n",
    "final_selection = []\n",
    "final_selection_before_M3 = []\n",
    "\n",
    "initial_scores_M3 = []\n",
    "initial_scores_M1 = []\n",
    "initial_scores_M2 = []\n",
    "initial_scores_M2_all = []\n",
    "index_selection_M1 = []\n",
    "index_selection_M2 = []\n",
    "index_selection_M3 = []\n",
    "\n",
    "df_results_save_final = pd.DataFrame()\n",
    "df_results_save = pd.DataFrame()\n",
    "\n",
    "print(file_name)\n",
    "\n",
    "for n_features in best_features_num:\n",
    "    print('----------------------------- ')\n",
    "    print('---Number of Features %0.0f-----' % n_features)\n",
    "    print('----------------------------- ')\n",
    "    print('\\n')\n",
    "\n",
    "    groups = wrist_df[\"Subject\"].values\n",
    "    logo = LeaveOneGroupOut()\n",
    "    logo.get_n_splits(wrist_X, wrist_Y, groups)\n",
    "    LeaveOneGroupOut()\n",
    "    f1_array = []\n",
    "\n",
    "    for item, item_name in zip(alg_array, alg_array_names): \n",
    "        print('\\n########### ' + str(item_name) + ' ##########')\n",
    "        subject_number = 1\n",
    "\n",
    "        for train_index, test_index in logo.split(wrist_X, wrist_Y, groups):\n",
    "            X_train_original, X_test_original = wrist_X[train_index], wrist_X[test_index]\n",
    "            y_train_original, y_test_original = wrist_Y[train_index], wrist_Y[test_index]\n",
    "\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            X_train_original = min_max_scaler.fit_transform(X_train_original)\n",
    "            X_test_original = min_max_scaler.transform(X_test_original)\n",
    "\n",
    "            # Baseline\n",
    "            baseline_model = item.fit(X_train_original, y_train_original)\n",
    "            baseline_pred = baseline_model.predict(X_test_original)\n",
    "\n",
    "            # DataFrame appendings\n",
    "            subject_name_df.append(\"User \" + str(subject_number))\n",
    "            item_array_df.append(item_name)\n",
    "            result_baseline_df.append(accuracy_score(y_test_original, baseline_pred) * 100)\n",
    "\n",
    "            print(\"\\n\\n-------------------------- Subject \" + str(subject_number) +\n",
    "                  \" - F1 baseline max features %.4f \" % (f1_score(y_test_original, baseline_pred, average='macro')))\n",
    "            print(\"-------------------------- Subject \" + str(subject_number) +\n",
    "                  \" - Acc baseline max features %.4f \" % (accuracy_score(y_test_original, baseline_pred)))\n",
    "\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            X_train_original = min_max_scaler.fit_transform(X_train_original)\n",
    "            X_test_original = min_max_scaler.transform(X_test_original)\n",
    "\n",
    "            for n in range(n_times):\n",
    "                sss = StratifiedShuffleSplit(n_splits=1, test_size=proportion_test, random_state=n)\n",
    "                sss.get_n_splits(X_test_original, y_test_original)\n",
    "\n",
    "                for first_index, second_index in sss.split(X_test_original, y_test_original):\n",
    "                    X_test_first, X_test_second = X_test_original[first_index], X_test_original[second_index]\n",
    "                    y_test_first, y_test_second = y_test_original[first_index], y_test_original[second_index]\n",
    "\n",
    "                    ############################################### ------------------------------ ROUND ONE\n",
    "                    ###########################################################################################\n",
    "                    #--------------------------- MODEL 1 ------------------------------------------------------\n",
    "                    ###########################################################################################\n",
    "                    index_selection, df_thres_1, clasif_report_M1, errors_selection_M1, score_selection_M1 = classif_model(\n",
    "                        item, n_features_m1, X_train_original, X_test_first, y_train_original, y_test_first, log_loss_T,\n",
    "                        final_model=False)\n",
    "                    index_classified = df_thres_1.index.values.tolist()\n",
    "                    X_test_classifed_M1 = X_test_first[index_classified]  # Classified\n",
    "                    y_test_classifed_M1 = y_test_first[index_classified]  # Classified\n",
    "                    X_test_selection_M1 = X_test_first[index_selection]  # pending \n",
    "                    y_test_selection_M1 = y_test_first[index_selection]  # pending \n",
    "\n",
    "                    ###########################################################################################\n",
    "                    #--------------------------- MODEL 2 ------------------------------------------------------\n",
    "                    ###########################################################################################\n",
    "                    n_samples = X_test_selection_M1.shape[0]\n",
    "                    if n_samples > 0:\n",
    "                        index_selection, df_thres_2, clasif_report_M2, errors_selection_M2, score_selection_M2 = classif_model(\n",
    "                            item, n_features_m2, X_train_original, X_test_selection_M1, y_train_original, y_test_selection_M1,\n",
    "                            log_loss_T_sel, final_model=False)\n",
    "                        X_test_selection_M2 = X_test_selection_M1[index_selection]  # pending \n",
    "                        y_test_selection_M2 = y_test_selection_M1[index_selection]  # pending \n",
    "                        index_classified = df_thres_2.index.values.tolist()\n",
    "                        X_test_classifed_M2 = X_test_selection_M1[index_classified]  # Classified\n",
    "                        y_test_classifed_M2 = y_test_selection_M1[index_classified]  # Classified\n",
    "                    else:\n",
    "                        print(\"R1 2 ----------------------------------Number of pending instances not enough to perform Model classification\")\n",
    "                        df_thres_2 = pd.DataFrame()\n",
    "                        index_selection_M2 = index_selection\n",
    "                        X_test_selection_M2 = X_test_selection_M1[index_selection]  # pending \n",
    "                        y_test_selection_M2 = y_test_selection_M1[index_selection]  # pending \n",
    "\n",
    "                    # Recapitulate model 2\n",
    "                    M2_all = [df_thres_1, df_thres_2]\n",
    "                    final_df_M2 = pd.concat(M2_all)\n",
    "                    selection_M2_all = final_df_M2.shape[0]\n",
    "\n",
    "                    ###########################################################################################\n",
    "                    #--------------------------- MODEL 3 ------------------------------------------------------ \n",
    "                    ###########################################################################################\n",
    "                    n_samples = X_test_selection_M2.shape[0]\n",
    "                    if n_samples > 0:\n",
    "                        index_selection, df_thres_3, clasif_report_M3, errors_selection_M3, score_selection_M2 = classif_model(\n",
    "                            item, n_features_m3, X_train_original, X_test_selection_M2, y_train_original, y_test_selection_M2,\n",
    "                            log_loss_T_sel, final_model=True)\n",
    "                    else:\n",
    "                        print(\" R1 3 Number of pending instances not enough to perform Model3 classification\")\n",
    "                        df_thres_3 = pd.DataFrame()\n",
    "\n",
    "                    ##########################################################################################\n",
    "                    #--------------------------- FINAL MODEL---------------------------------------------------\n",
    "                    ###########################################################################################\n",
    "                    final = [df_thres_1, df_thres_2, df_thres_3]\n",
    "                    final_df = pd.concat(final)\n",
    "                    selection_M3 = df_thres_3.shape[0]\n",
    "\n",
    "                    ###########################################################################################\n",
    "                    #____________________________________________________________________________________________\n",
    "                    #                               Second ROUND \n",
    "                    #____________________________________________________________________________________________\n",
    "                    ###########################################################################################\n",
    "                    # Instancias añadidas a train\n",
    "                    add_train.append(X_test_selection_M2.shape[0])\n",
    "                    X_train_second = np.concatenate((X_train_original, X_test_selection_M2), axis=0)\n",
    "                    y_train_second = np.concatenate((y_train_original, y_test_selection_M2), axis=0)\n",
    "\n",
    "                    ###########################################################################################\n",
    "                    #--------------------------- MODEL 1 (Original Train) ------------------------------------------------------\n",
    "                    ###########################################################################################\n",
    "                    index_selection, df_thres_1, clasif_report_M1, errors_selection_M1, score_selection_M1 = classif_model(\n",
    "                        item, n_features_m1, X_train_original, X_test_second, y_train_original, y_test_second, log_loss_T,\n",
    "                        final_model=False)\n",
    "                    X_test_selection_M1 = X_test_second[index_selection]  # pending \n",
    "                    y_test_selection_M1 = y_test_second[index_selection]  # pending \n",
    "\n",
    "                    ###########################################################################################\n",
    "                    #--------------------------- MODEL 2 (Original Train) ------------------------------------------------------\n",
    "                    ###########################################################################################\n",
    "                    n_samples = X_test_selection_M1.shape[0]\n",
    "                    if n_samples > 0:\n",
    "                        index_selection, df_thres_2, clasif_report_M2, errors_selection_M2, score_selection_M2 = classif_model(\n",
    "                            item, n_features_m2, X_train_original, X_test_selection_M1, y_train_original, y_test_selection_M1,\n",
    "                            log_loss_T_sel, final_model=False)\n",
    "                        X_test_selection_M2 = X_test_selection_M1[index_selection]  # pending \n",
    "                        y_test_selection_M2 = y_test_selection_M1[index_selection]  # pending \n",
    "                    else:\n",
    "                        print(\"R 2 Orig Number of pending instances not enough to perform Model classification\")\n",
    "                        df_thres_2 = pd.DataFrame()\n",
    "                        index_selection_M2 = index_selection\n",
    "                        X_test_selection_M2 = X_test_selection_M1[index_selection]  # pending \n",
    "                        y_test_selection_M2 = y_test_selection_M1[index_selection]  # pending \n",
    "\n",
    "                    # Recapitulate model 2\n",
    "                    M2_all = [df_thres_1, df_thres_2]\n",
    "                    final_df_M2 = pd.concat(M2_all)\n",
    "\n",
    "                    acc_M1_M2.append(accuracy_score(final_df_M2[\"True_label\"].values, final_df_M2[\"Pred_label\"].values) * 100)\n",
    "                    pending_M1_M2.append(X_test_selection_M2.shape[0])\n",
    "\n",
    "                    ###########################################################################################\n",
    "                    #--------------------------- MODEL 3 (Original Train) ------------------------------------------------------\n",
    "                    ###########################################################################################\n",
    "                    n_samples = X_test_selection_M2.shape[0]\n",
    "                    if n_samples > 0:\n",
    "                        index_selection, df_thres_3, clasif_report_M3, errors_selection_M3, score_selection_M2 = classif_model(\n",
    "                            item, n_features_m3, X_train_original, X_test_selection_M2, y_train_original, y_test_selection_M2,\n",
    "                            log_loss_T_sel, final_model=True)\n",
    "                    else:\n",
    "                        print(\"R2 3 Orig Number of pending instances not enough to perform Model3 classification\")\n",
    "                        df_thres_3 = pd.DataFrame()\n",
    "\n",
    "                    final = [df_thres_1, df_thres_2, df_thres_3]\n",
    "                    final_df = pd.concat(final)\n",
    "                    acc_M3.append(accuracy_score(final_df[\"True_label\"].values, final_df[\"Pred_label\"].values) * 100)\n",
    "\n",
    "                    ###########################################################################################\n",
    "                    #--------------------------- MODEL 1 (Personalzed Train) ------------------------------------------------------\n",
    "                    ###########################################################################################\n",
    "                    index_selection, df_thres_1, clasif_report_M1, errors_selection_M1, score_selection_M1 = classif_model(\n",
    "                        item, n_features_m1, X_train_second, X_test_second, y_train_second, y_test_second, log_loss_T,\n",
    "                        final_model=False)\n",
    "                    X_test_selection_M1 = X_test_second[index_selection]  # pending \n",
    "                    y_test_selection_M1 = y_test_second[index_selection]  # pending \n",
    "\n",
    "                    ###########################################################################################\n",
    "                    #--------------------------- MODEL 2 (Personalized Train) ------------------------------------------------------\n",
    "                    ###########################################################################################\n",
    "                    n_samples = X_test_selection_M1.shape[0]\n",
    "                    if n_samples > 0:\n",
    "                        index_selection, df_thres_2, clasif_report_M2, errors_selection_M2, score_selection_M2 = classif_model(\n",
    "                            item, n_features_m2, X_train_second, X_test_selection_M1, y_train_second, y_test_selection_M1,\n",
    "                            log_loss_T_sel, final_model=False)\n",
    "                        X_test_selection_M2 = X_test_selection_M1[index_selection]  # pending \n",
    "                        y_test_selection_M2 = y_test_selection_M1[index_selection]  # pending \n",
    "                    else:\n",
    "                        print(\"R2 2 New Number of pending instances not enough to perform Model classification\")\n",
    "                        df_thres_2 = pd.DataFrame()\n",
    "                        index_selection_M2 = index_selection\n",
    "                        X_test_selection_M2 = X_test_selection_M1[index_selection]  # pending \n",
    "                        y_test_selection_M2 = y_test_selection_M1[index_selection]  # pending \n",
    "\n",
    "                    M2_all = [df_thres_1, df_thres_2]\n",
    "                    final_df_M2 = pd.concat(M2_all)\n",
    "\n",
    "                    second_round_acc_M1_M2.append(accuracy_score(final_df_M2[\"True_label\"].values, final_df_M2[\"Pred_label\"].values) * 100)\n",
    "                    pending_M1_M2_second_round.append(X_test_selection_M2.shape[0])\n",
    "\n",
    "                    ###########################################################################################\n",
    "                    #--------------------------- MODEL 3 (Personalized Train) ------------------------------------------------------\n",
    "                    ###########################################################################################\n",
    "                    n_samples = X_test_selection_M2.shape[0]\n",
    "                    if n_samples > 0:\n",
    "                        index_selection, df_thres_3, clasif_report_M3, errors_selection_M3, score_selection_M2 = classif_model(\n",
    "                            item, n_features_m3, X_train_second, X_test_selection_M2, y_train_second, y_test_selection_M2,\n",
    "                            log_loss_T_sel, final_model=True)\n",
    "                    else:\n",
    "                        print(\" R2 3 New Number of pending instances not enough to perform Model3 classification\")\n",
    "                        df_thres_3 = pd.DataFrame()\n",
    "\n",
    "                    final = [df_thres_1, df_thres_2, df_thres_3]\n",
    "                    final_df = pd.concat(final)\n",
    "                    second_round_acc_M3.append(accuracy_score(final_df[\"True_label\"].values, final_df[\"Pred_label\"].values) * 100)\n",
    "\n",
    "            subject_number = subject_number + 1\n",
    "\n",
    "            print(\"\\n --- primera ronda \")\n",
    "            print(\"Modelo 1 y 2\")\n",
    "            print(acc_M1_M2)\n",
    "            print(np.mean(acc_M1_M2))\n",
    "            print(\"Classificadas \" + str(np.mean(pending_M1_M2)))\n",
    "            print(\"\\nModelo 3\")\n",
    "            print(acc_M3)\n",
    "            print(np.mean(acc_M3))\n",
    "            print(\"\\n --- segunda ronda \")\n",
    "            print(\"Modelo 1 y 2\")\n",
    "            print(second_round_acc_M1_M2)\n",
    "            print(np.mean(second_round_acc_M1_M2))\n",
    "            print(\"\\nModelo 3\")\n",
    "            print(second_round_acc_M3)\n",
    "            print(np.mean(second_round_acc_M3))\n",
    "            print(\"Classificadas \" + str(np.mean(pending_M1_M2_second_round)))\n",
    "\n",
    "            add_train_df.append(np.mean(add_train))  # Añadidas al train\n",
    "            acc_M1_M2_df.append(np.mean(acc_M1_M2))\n",
    "            acc_M3_df.append(np.mean(acc_M3))\n",
    "            second_round_acc_M1_M2_df.append(np.mean(second_round_acc_M1_M2))\n",
    "            second_round_acc_M3_df.append(np.mean(second_round_acc_M3))\n",
    "            pending_M1_M2_df.append(np.mean(pending_M1_M2))\n",
    "            pending_M1_M2_second_round_df.append(np.mean(pending_M1_M2_second_round))\n",
    "\n",
    "            acc_M1_M2 = []\n",
    "            acc_M3 = []\n",
    "            second_round_acc_M1_M2 = []\n",
    "            second_round_acc_M3 = []\n",
    "            pending_M1_M2 = []\n",
    "            pending_M1_M2_second_round = []\n",
    "\n",
    "            error_dict_NO_S[item_name] = []\n",
    "            error_dict_thres_NO_S[item_name] = []\n",
    "            initial_scores_M1 = []\n",
    "            initial_scores_M2 = []\n",
    "            initial_scores_M3 = []\n",
    "            threshold[item_name] = []\n",
    "            error_dict[item_name] = []\n",
    "            error_dict_thres[item_name] = []\n",
    "            array_dict_pre[item_name] = []\n",
    "            array_dict_pre_thres[item_name] = []\n",
    "            error_dict[item_name] = []\n",
    "            error_dict_thres[item_name] = []\n",
    "            array_dict_pre[item_name] = []\n",
    "            array_dict_pre_thres[item_name] = []\n",
    "\n",
    "            final_selection_before_M3 = []\n",
    "            originalclass = []\n",
    "            predictedclass = []\n",
    "            original_class = []\n",
    "            predicted_class = []\n",
    "            score_final = []\n",
    "            score_final_M2 = []\n",
    "            final_selection = []\n",
    "            final_selection_before_M3 = []\n",
    "\n",
    "            initial_scores_M3 = []\n",
    "            initial_scores_M1 = []\n",
    "            initial_scores_M2 = []\n",
    "            initial_scores_M2_all = []\n",
    "            index_selection_M1 = []\n",
    "            index_selection_M2 = []\n",
    "            index_selection_M3 = []\n",
    "\n",
    "        df_results_save[\"User name\"] = subject_name_df\n",
    "        df_results_save[\"Alg\"] = item_array_df\n",
    "        df_results_save[\"Acc baseline\"] = result_baseline_df\n",
    "        df_results_save[\"Add train\"] = add_train_df\n",
    "        df_results_save[\"Acc M1+M2 initial\"] = acc_M1_M2_df\n",
    "        df_results_save[\"Acc all initial\"] = acc_M3_df\n",
    "        df_results_save[\"Pending inicial\"] = pending_M1_M2_df\n",
    "        df_results_save[\"Acc M1+M2 second\"] = second_round_acc_M1_M2_df\n",
    "        df_results_save[\"Acc all second\"] = second_round_acc_M3_df\n",
    "        df_results_save[\"Pending second\"] = pending_M1_M2_second_round_df\n",
    "\n",
    "        print(item_array_df)\n",
    "        appended_data.append(df_results_save)\n",
    "        print(\"Append_data\")\n",
    "        print(appended_data)\n",
    "\n",
    "        subject_name_df = []\n",
    "        item_array_df = []\n",
    "        result_baseline_df = []\n",
    "        add_train_df = []\n",
    "        acc_M1_M2_df = []\n",
    "        acc_M3_df = []\n",
    "        second_round_acc_M1_M2_df = []\n",
    "        second_round_acc_M3_df = []\n",
    "        pending_M1_M2_df = []\n",
    "        pending_M1_M2_second_round_df = []\n",
    "\n",
    "        df_results_save_final = pd.concat([df_results_save_final, df_results_save], axis=1)\n",
    "        print(\"df_final\")\n",
    "        print(df_results_save_final)\n",
    "\n",
    "    appended_data = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Final Results to an Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the file name for the XLSX file\n",
    "print(df_results_save_final.head())\n",
    "with pd.ExcelWriter(file_name, engine='openpyxl') as writer:\n",
    "    df_results_save_final.to_excel(writer, sheet_name='Sheet1', startrow=1, header=True, index=False)\n",
    "\n",
    "print(\"Saved to \" + str(file_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-cascade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
